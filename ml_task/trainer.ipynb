{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataLoader import VoicePassingDataloader\n",
    "from model import VoicePassingModel\n",
    "from transformers import DistilBertTokenizer, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    'model' : VoicePassingModel(),\n",
    "}\n",
    "\n",
    "class VoicePassingTrainer():\n",
    "\n",
    "    def __init__(self, model):\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.valid_loss_history = []\n",
    "        self.valid_acc_history = []\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "    def get_history(self):\n",
    "\n",
    "        history = {\n",
    "            \"train_loss\" : self.train_loss_history,\n",
    "            \"train_accuracy\" : self.train_acc_history,\n",
    "            \"valid_loss\" : self.valid_loss_history,\n",
    "            \"valid_accuracy\" : self.valid_acc_history\n",
    "        }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def train(self, num_epochs, train_loader, criterion, lr = 3e-5, valid_loader = None, reset_history = False):\n",
    "\n",
    "        if reset_history:\n",
    "            self.train_loss_history = []\n",
    "            self.train_acc_history = []\n",
    "            self.valid_loss_history = []\n",
    "            self.valid_acc_history = []\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = AdamW(params = self.model.parameters(), lr = lr, correct_bias=False)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_one_epoch(epoch_idx, train_loader)\n",
    "\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "\n",
    "            if valid_loader:\n",
    "                valid_loss, valid_acc = self.validate(epoch_idx, valid_loader)\n",
    "\n",
    "                self.valid_loss_history.append(valid_loss)\n",
    "                self.valid_acc_history.append(valid_acc)\n",
    "\n",
    "    def train_one_epoch(self, index, train_loader, verbose = True):\n",
    "\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_n_probs = 0\n",
    "\n",
    "        for X, y in tqdm(train_loader, desc=\"batch\", leave= True):\n",
    "\n",
    "            X = self.tokenizer(\n",
    "                text = X,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding = \"max_length\",\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            y = y.squeeze().to(self.device)\n",
    "\n",
    "            pred = self.model(X)\n",
    "        \n",
    "            loss = self.criterion(pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred_labels = pred.argmax(axis = 1)\n",
    "            n_correct = len(torch.where(pred_labels == y)[0])\n",
    "\n",
    "            train_correct += n_correct\n",
    "            train_n_probs += len(y)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"EPOCH {index+1} Loss : {train_loss : .4f}\")\n",
    "            print(pred[-4:])\n",
    "            print(y[-4:])\n",
    "\n",
    "        train_acc = (train_correct / train_n_probs) * 100\n",
    "        \n",
    "        return train_loss, train_acc\n",
    "\n",
    "    def test_a_sentence(self, text):\n",
    "\n",
    "        X = self.tokenizer(\n",
    "                text = text,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding = \"max_length\",\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(self.device)\n",
    "        \n",
    "        pred = self.model(X)\n",
    "        return pred\n",
    "    \n",
    "    def validate(self, index, valid_loader, verbose = True):\n",
    "\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "        valid_n_probs = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        for X, y in tqdm(valid_loader, desc=\"batch\", leave= True):\n",
    "\n",
    "            X = self.tokenizer(\n",
    "                text = X,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding = \"max_length\",\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            y = y.squeeze().to(self.device)\n",
    "\n",
    "            pred = self.model(X)        \n",
    "            loss = self.criterion(pred, y)\n",
    "\n",
    "            pred_labels = pred.argmax(axis = 1)\n",
    "            n_correct = len(torch.where(pred_labels == y)[0])\n",
    "\n",
    "            valid_correct += n_correct\n",
    "            valid_n_probs += len(y)\n",
    "\n",
    "        valid_acc = (valid_correct / valid_n_probs) * 100\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"EPOCH {index+1} Loss : {valid_loss : .4f}, Acc : {valid_acc : .4f}\")\n",
    "            print(pred[-4:])\n",
    "            print(y[-4:])\n",
    "        \n",
    "        return valid_loss, valid_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trainer = VoicePassingTrainer(model = VoicePassingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\S08P31A607\\venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "batch:   6%|â–Œ         | 19/310 [00:11<02:52,  1.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m5\u001b[39;49m, VoicePassingDataloader(), criterion \u001b[39m=\u001b[39;49m criterion, valid_loader\u001b[39m=\u001b[39;49mVoicePassingDataloader(test \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m))\n",
      "Cell \u001b[1;32mIn[50], line 44\u001b[0m, in \u001b[0;36mVoicePassingTrainer.train\u001b[1;34m(self, num_epochs, train_loader, criterion, lr, valid_loader, reset_history)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 44\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch_idx, train_loader)\n\u001b[0;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_acc_history\u001b[39m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[1;32mIn[50], line 79\u001b[0m, in \u001b[0;36mVoicePassingTrainer.train_one_epoch\u001b[1;34m(self, index, train_loader, verbose)\u001b[0m\n\u001b[0;32m     76\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(pred, y)\n\u001b[0;32m     77\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 79\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     82\u001b[0m pred_labels \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39margmax(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\Desktop\\S08P31A607\\venv\\lib\\site-packages\\torch\\_tensor.py:428\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_tensor_str\u001b[39m.\u001b[39m_str(\u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents)\n\u001b[1;32m--> 428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m, gradient\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, retain_graph\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, inputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    430\u001b[0m ):\n\u001b[0;32m    431\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[39m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(5, VoicePassingDataloader(), criterion = criterion, valid_loader=VoicePassingDataloader(test = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
