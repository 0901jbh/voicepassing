{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\S08P31A607\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataLoader import small_train_loader\n",
    "from model import VoicePassingModel\n",
    "from transformers import DistilBertTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterer = iter(small_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = tokenizer(\n",
    "    text = X,\n",
    "    add_special_tokens = True,\n",
    "    max_length = 512,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = VoicePassingModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_epoch in tqdm(range(10)):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (X, y) in enumerate(small_train_loader):\n",
    "\n",
    "        X_ = tokenizer(\n",
    "            text = X,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 512,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_tensors = \"pt\"\n",
    "            ).to(device)\n",
    "        \n",
    "        pred = model(X_)\n",
    "\n",
    "        loss = criterion(pred, y.squeeze().to(device))\n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 1:\n",
    "            break\n",
    "\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.item>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataLoader import small_train_loader\n",
    "from model import VoicePassingModel\n",
    "from transformers import DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "trainer_config = {\n",
    "    'model' : VoicePassingModel(),\n",
    "}\n",
    "\n",
    "class VoicePassingTrainer():\n",
    "\n",
    "    def __init__(self, model):\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "    def train(self, num_epochs, train_loader, criterion, lr = 0.003, hist = False):\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = lr)\n",
    "        self.model.train()\n",
    "\n",
    "        for num_epoch in tqdm(range(num_epochs), desc=\"EPOCH\", leave = True):\n",
    "            result = self.train_one_epoch(num_epoch, train_loader, hist = hist)\n",
    "            \n",
    "\n",
    "    def train_one_epoch(self, index, train_loader, verbose = True, hist = False):\n",
    "\n",
    "        train_loss = 0\n",
    "\n",
    "        for X, y in tqdm(train_loader, desc=\"batch\", leave=False):\n",
    "\n",
    "            X = self.tokenizer(\n",
    "                text = X,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding = \"max_length\",\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            y = y.squeeze().to(self.device)\n",
    "\n",
    "            pred = self.model(X)\n",
    "            loss = self.criterion(pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"EPOCH {index+1} Loss : {train_loss : .4f}\")\n",
    "            print(pred)\n",
    "        \n",
    "        if hist:\n",
    "            return train_loss\n",
    "\n",
    "    def test_a_sentence(self, text):\n",
    "\n",
    "        X = self.tokenizer(\n",
    "                text = text,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding = \"max_length\",\n",
    "                truncation = True,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(self.device)\n",
    "        \n",
    "        pred = self.model(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trainer = VoicePassingTrainer(VoicePassingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:   3%|▎         | 1/30 [00:02<01:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 Loss :  26.3771\n",
      "tensor([[ -0.2854,   0.6702,   0.7785,  -0.9317],\n",
      "        [ 10.8365,   0.8576,  -3.2630, -15.8015],\n",
      "        [ -0.2854,   0.6702,   0.7785,  -0.9317],\n",
      "        [ -0.2854,   0.6702,   0.7785,  -0.9317]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:   7%|▋         | 2/30 [00:04<00:56,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 Loss :  27.4193\n",
      "tensor([[ -0.2730,   0.6584,   0.7910,  -0.9442],\n",
      "        [  8.0653,   0.6971,  -2.3724, -11.6767],\n",
      "        [ -0.2730,   0.6584,   0.7910,  -0.9442],\n",
      "        [ -0.2730,   0.6584,   0.7910,  -0.9442]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  10%|█         | 3/30 [00:05<00:52,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3 Loss :  26.1112\n",
      "tensor([[-0.2598,  0.6458,  0.8033, -0.9568],\n",
      "        [-0.2598,  0.6458,  0.8033, -0.9568],\n",
      "        [-0.2598,  0.6458,  0.8033, -0.9568],\n",
      "        [-0.2598,  0.6458,  0.8033, -0.9568]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  13%|█▎        | 4/30 [00:07<00:49,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4 Loss :  26.1357\n",
      "tensor([[-0.2459,  0.6327,  0.8155, -0.9693],\n",
      "        [-0.2459,  0.6327,  0.8155, -0.9693],\n",
      "        [-0.2459,  0.6327,  0.8155, -0.9693],\n",
      "        [-0.2459,  0.6327,  0.8155, -0.9693]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  17%|█▋        | 5/30 [00:09<00:47,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5 Loss :  30.3556\n",
      "tensor([[-0.2310,  0.6190,  0.8275, -0.9819],\n",
      "        [ 6.2667,  0.7614, -1.9548, -8.4692],\n",
      "        [-0.2310,  0.6190,  0.8275, -0.9819],\n",
      "        [-0.2310,  0.6190,  0.8275, -0.9819]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  20%|██        | 6/30 [00:11<00:45,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6 Loss :  29.0475\n",
      "tensor([[-0.2146,  0.6041,  0.8390, -0.9945],\n",
      "        [ 4.0603,  0.7505, -1.0529, -5.7448],\n",
      "        [-0.2146,  0.6041,  0.8390, -0.9945],\n",
      "        [-0.2146,  0.6041,  0.8390, -0.9945]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  23%|██▎       | 7/30 [00:13<00:43,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7 Loss :  27.1404\n",
      "tensor([[-0.1973,  0.5884,  0.8501, -1.0072],\n",
      "        [ 6.3766,  0.8757, -2.1455, -8.0461],\n",
      "        [-0.1973,  0.5884,  0.8501, -1.0072],\n",
      "        [-0.1973,  0.5884,  0.8501, -1.0072]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  27%|██▋       | 8/30 [00:15<00:41,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8 Loss :  29.0039\n",
      "tensor([[-0.1792,  0.5725,  0.8606, -1.0199],\n",
      "        [-0.1792,  0.5725,  0.8606, -1.0199],\n",
      "        [-0.1792,  0.5725,  0.8606, -1.0199],\n",
      "        [-0.1792,  0.5725,  0.8606, -1.0199]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  30%|███       | 9/30 [00:17<00:39,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9 Loss :  26.9344\n",
      "tensor([[-0.1606,  0.5565,  0.8706, -1.0326],\n",
      "        [ 8.3001,  0.9626, -3.1834, -9.3918],\n",
      "        [-0.1606,  0.5565,  0.8706, -1.0326],\n",
      "        [-0.1606,  0.5565,  0.8706, -1.0326]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  33%|███▎      | 10/30 [00:19<00:37,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10 Loss :  26.4360\n",
      "tensor([[-0.1420,  0.5412,  0.8801, -1.0453],\n",
      "        [-0.1420,  0.5412,  0.8801, -1.0453],\n",
      "        [-0.1420,  0.5412,  0.8801, -1.0453],\n",
      "        [-0.1420,  0.5412,  0.8801, -1.0453]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  37%|███▋      | 11/30 [00:21<00:35,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11 Loss :  27.1341\n",
      "tensor([[-0.1236,  0.5276,  0.8891, -1.0580],\n",
      "        [ 9.5194,  0.8938, -3.9302, -9.8113],\n",
      "        [-0.1236,  0.5276,  0.8891, -1.0580],\n",
      "        [-0.1236,  0.5276,  0.8891, -1.0580]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  40%|████      | 12/30 [00:22<00:33,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12 Loss :  25.5496\n",
      "tensor([[-0.1062,  0.5170,  0.8979, -1.0707],\n",
      "        [-0.1062,  0.5170,  0.8979, -1.0707],\n",
      "        [-0.1062,  0.5170,  0.8979, -1.0707],\n",
      "        [-0.1062,  0.5170,  0.8979, -1.0707]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  43%|████▎     | 13/30 [00:24<00:32,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13 Loss :  25.8045\n",
      "tensor([[-0.0894,  0.5098,  0.9063, -1.0834],\n",
      "        [-0.0894,  0.5098,  0.9063, -1.0834],\n",
      "        [-0.0894,  0.5098,  0.9063, -1.0834],\n",
      "        [-0.0894,  0.5098,  0.9063, -1.0834]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH:  47%|████▋     | 14/30 [00:26<00:30,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14 Loss :  27.0306\n",
      "tensor([[ -0.0734,   0.5069,   0.9145,  -1.0960],\n",
      "        [ 15.3998,   2.0470,  -7.3409, -13.7321],\n",
      "        [ -0.0734,   0.5069,   0.9145,  -1.0960],\n",
      "        [ -0.0734,   0.5069,   0.9145,  -1.0960]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train(30, small_train_loader, criterion, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2973,  0.6820,  0.7665, -0.9197]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test_a_sentence(\"예, 그 증거로 제출하면 채택 될 때 사용됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('김승재라고 아시는 분이십니까',\n",
       "  '혹시 타인에게 통장  판매하신 적 있으신가요?',\n",
       "  '예, 그 증거로 제출하면 채택 될 때 사용됩니다.',\n",
       "  '그러시면 칠십하루 년생 사내 강자 상자 호차 강상호란 사람은 아십니까'),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(small_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
